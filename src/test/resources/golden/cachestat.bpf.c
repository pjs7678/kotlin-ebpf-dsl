// AUTO-GENERATED by kotlin-ebpf-dsl â€” do not edit

#include "vmlinux.h"
#include <bpf/bpf_helpers.h>
#include <bpf/bpf_tracing.h>
#include <bpf/bpf_core_read.h>

char LICENSE[] SEC("license") = "GPL";

struct cgroup_key {
    __u64 cgroup_id;
};

struct cache_stats {
    __u64 accesses;
    __u64 additions;
    __u64 dirtied;
    __u64 buf_dirtied;
};

struct {
    __uint(type, BPF_MAP_TYPE_LRU_HASH);
    __uint(max_entries, 10240);
    __type(key, struct cgroup_key);
    __type(value, struct cache_stats);
} cache_stats SEC(".maps");

SEC("kprobe/mark_page_accessed")
int kprobe_mark_page_accessed(struct pt_regs *ctx)
{
    __u64 cgroup_id = bpf_get_current_cgroup_id();
    struct cgroup_key var_0 = {};
    var_0.cgroup_id = cgroup_id;
    struct cache_stats *entry_1 = bpf_map_lookup_elem(&cache_stats, &var_0);
    if (entry_1) {
        __sync_fetch_and_add(&entry_1->accesses, 1ULL);
    } else {
        struct cache_stats var_2 = {};
        var_2.accesses = 1ULL;
        bpf_map_update_elem(&cache_stats, &var_0, &var_2, 1);
    }
    return 0;
}

SEC("kprobe/add_to_page_cache_lru")
int kprobe_add_to_page_cache_lru(struct pt_regs *ctx)
{
    __u64 cgroup_id = bpf_get_current_cgroup_id();
    struct cgroup_key var_0 = {};
    var_0.cgroup_id = cgroup_id;
    struct cache_stats *entry_1 = bpf_map_lookup_elem(&cache_stats, &var_0);
    if (entry_1) {
        __sync_fetch_and_add(&entry_1->additions, 1ULL);
    } else {
        struct cache_stats var_2 = {};
        var_2.additions = 1ULL;
        bpf_map_update_elem(&cache_stats, &var_0, &var_2, 1);
    }
    return 0;
}

SEC("kprobe/account_page_dirtied")
int kprobe_account_page_dirtied(struct pt_regs *ctx)
{
    __u64 cgroup_id = bpf_get_current_cgroup_id();
    struct cgroup_key var_0 = {};
    var_0.cgroup_id = cgroup_id;
    struct cache_stats *entry_1 = bpf_map_lookup_elem(&cache_stats, &var_0);
    if (entry_1) {
        __sync_fetch_and_add(&entry_1->dirtied, 1ULL);
    } else {
        struct cache_stats var_2 = {};
        var_2.dirtied = 1ULL;
        bpf_map_update_elem(&cache_stats, &var_0, &var_2, 1);
    }
    return 0;
}

SEC("kprobe/mark_buffer_dirty")
int kprobe_mark_buffer_dirty(struct pt_regs *ctx)
{
    __u64 cgroup_id = bpf_get_current_cgroup_id();
    struct cgroup_key var_0 = {};
    var_0.cgroup_id = cgroup_id;
    struct cache_stats *entry_1 = bpf_map_lookup_elem(&cache_stats, &var_0);
    if (entry_1) {
        __sync_fetch_and_add(&entry_1->buf_dirtied, 1ULL);
    } else {
        struct cache_stats var_2 = {};
        var_2.buf_dirtied = 1ULL;
        bpf_map_update_elem(&cache_stats, &var_0, &var_2, 1);
    }
    return 0;
}
